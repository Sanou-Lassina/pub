# Performence.py
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import StandardScaler
import joblib
import time

def main():
    st.markdown("""
    <style>
    .performance-header {
        font-size: 2.5rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
        font-weight: bold;
    }
    </style>
    """, unsafe_allow_html=True)
    
    st.markdown('<h1 class="performance-header">ğŸ¤– Performance des ModÃ¨les</h1>', unsafe_allow_html=True)
    
    if st.session_state.df is None:
        st.warning("âš ï¸ Veuillez d'abord importer des donnÃ©es dans la page d'Accueil")
        return
    
    df = st.session_state.df
    
    # VÃ©rification des colonnes requises
    required_cols = ['TV_Ad_Budget', 'Radio_Ad_Budget', 'Newspaper_Ad_Budget', 'Sales']
    missing_cols = [col for col in required_cols if col not in df.columns]
    
    if missing_cols:
        st.error(f"âŒ Colonnes manquantes: {', '.join(missing_cols)}")
        st.info("Veuillez vous assurer que votre dataset contient les colonnes: TV_Ad_Budget, Radio_Ad_Budget, Newspaper_Ad_Budget, Sales")
        return
    
    # Onglets pour l'analyse des performances
    tab1, tab2, tab3, tab4 = st.tabs([
        "ğŸ¯ EntraÃ®nement", 
        "ğŸ“Š Comparaison", 
        "ğŸ† Recommandations", 
        "ğŸ’¾ ModÃ¨les"
    ])
    
    with tab1:
        show_model_training(df)
    
    with tab2:
        show_model_comparison()
    
    with tab3:
        show_recommendations()
    
    with tab4:
        show_model_management()

def show_model_training(df):
    st.header("ğŸ¯ EntraÃ®nement des ModÃ¨les")
    
    # Configuration de l'entraÃ®nement
    st.subheader("âš™ï¸ Configuration de l'EntraÃ®nement")
    
    col1, col2 = st.columns(2)
    
    with col1:
        test_size = st.slider("Taille du jeu de test (%)", 10, 40, 20) / 100
        random_state = st.number_input("Random State", value=42)
    
    with col2:
        features = st.multiselect(
            "Features Ã  utiliser:",
            ['TV_Ad_Budget', 'Radio_Ad_Budget', 'Newspaper_Ad_Budget'],
            default=['TV_Ad_Budget', 'Radio_Ad_Budget', 'Newspaper_Ad_Budget']
        )
        
        target = 'Sales'
    
    # SÃ©lection des modÃ¨les
    st.subheader("ğŸ¤– SÃ©lection des ModÃ¨les")
    
    models_to_train = {}
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.checkbox("RÃ©gression LinÃ©aire", value=True):
            models_to_train['Linear Regression'] = LinearRegression()
        
        if st.checkbox("Ridge Regression"):
            alpha_ridge = st.number_input("Alpha Ridge", value=1.0, key="alpha_ridge")
            models_to_train['Ridge Regression'] = Ridge(alpha=alpha_ridge)
    
    with col2:
        if st.checkbox("Lasso Regression"):
            alpha_lasso = st.number_input("Alpha Lasso", value=1.0, key="alpha_lasso")
            models_to_train['Lasso Regression'] = Lasso(alpha=alpha_lasso)
        
        if st.checkbox("Random Forest"):
            n_estimators = st.number_input("N Estimators", value=100, key="n_estimators")
            models_to_train['Random Forest'] = RandomForestRegressor(n_estimators=n_estimators, random_state=random_state)
    
    with col3:
        if st.checkbox("Gradient Boosting"):
            learning_rate = st.number_input("Learning Rate", value=0.1, key="learning_rate")
            models_to_train['Gradient Boosting'] = GradientBoostingRegressor(
                n_estimators=100, learning_rate=learning_rate, random_state=random_state
            )
    
    # PrÃ©paration des donnÃ©es
    X = df[features]
    y = df[target]
    
    # Normalisation des features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # Split des donnÃ©es
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=test_size, random_state=random_state
    )
    
    # Bouton d'entraÃ®nement
    if st.button("ğŸš€ Lancer l'EntraÃ®nement", type="primary"):
        if not models_to_train:
            st.warning("âš ï¸ Veuillez sÃ©lectionner au moins un modÃ¨le")
            return
        
        st.session_state.models_trained = True
        st.session_state.models_results = {}
        st.session_state.scaler = scaler
        st.session_state.features = features
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # EntraÃ®nement des modÃ¨les
        for i, (name, model) in enumerate(models_to_train.items()):
            status_text.text(f"EntraÃ®nement de {name}...")
            
            # EntraÃ®nement du modÃ¨le
            start_time = time.time()
            model.fit(X_train, y_train)
            training_time = time.time() - start_time
            
            # PrÃ©dictions
            y_pred_train = model.predict(X_train)
            y_pred_test = model.predict(X_test)
            
            # Calcul des mÃ©triques
            train_metrics = {
                'R2': r2_score(y_train, y_pred_train),
                'RMSE': np.sqrt(mean_squared_error(y_train, y_pred_train)),
                'MAE': mean_absolute_error(y_train, y_pred_train),
                'MSE': mean_squared_error(y_train, y_pred_train)
            }
            
            test_metrics = {
                'R2': r2_score(y_test, y_pred_test),
                'RMSE': np.sqrt(mean_squared_error(y_test, y_pred_test)),
                'MAE': mean_absolute_error(y_test, y_pred_test),
                'MSE': mean_squared_error(y_test, y_pred_test)
            }
            
            # Stockage des rÃ©sultats
            st.session_state.models_results[name] = {
                'model': model,
                'train_metrics': train_metrics,
                'test_metrics': test_metrics,
                'training_time': training_time,
                'predictions': {
                    'train': y_pred_train,
                    'test': y_pred_test
                }
            }
            
            progress_bar.progress((i + 1) / len(models_to_train))
        
        status_text.text("âœ… EntraÃ®nement terminÃ©!")
        st.balloons()
    
    # Affichage des rÃ©sultats si l'entraÃ®nement est fait
    if st.session_state.get('models_trained', False):
        st.subheader("ğŸ“ˆ RÃ©sultats de l'EntraÃ®nement")
        
        for name, results in st.session_state.models_results.items():
            with st.expander(f"ğŸ“Š {name}", expanded=True):
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric("RÂ² Train", f"{results['train_metrics']['R2']:.4f}")
                    st.metric("RÂ² Test", f"{results['test_metrics']['R2']:.4f}")
                
                with col2:
                    st.metric("RMSE Train", f"{results['train_metrics']['RMSE']:.2f}")
                    st.metric("RMSE Test", f"{results['test_metrics']['RMSE']:.2f}")
                
                with col3:
                    st.metric("MAE Train", f"{results['train_metrics']['MAE']:.2f}")
                    st.metric("MAE Test", f"{results['test_metrics']['MAE']:.2f}")
                
                with col4:
                    st.metric("Temps d'entraÃ®nement", f"{results['training_time']:.2f}s")
                
                # Graphique des prÃ©dictions vs rÃ©elles
                fig = go.Figure()
                
                fig.add_trace(go.Scatter(
                    x=y_test, y=results['predictions']['test'],
                    mode='markers',
                    name='PrÃ©dictions vs RÃ©elles',
                    marker=dict(color='blue', opacity=0.6)
                ))
                
                # Ligne de perfection
                min_val = min(y_test.min(), results['predictions']['test'].min())
                max_val = max(y_test.max(), results['predictions']['test'].max())
                fig.add_trace(go.Scatter(
                    x=[min_val, max_val], y=[min_val, max_val],
                    mode='lines',
                    name='Ligne parfaite',
                    line=dict(color='red', dash='dash')
                ))
                
                fig.update_layout(
                    title=f"{name} - PrÃ©dictions vs Valeurs RÃ©elles",
                    xaxis_title="Valeurs RÃ©elles",
                    yaxis_title="PrÃ©dictions",
                    showlegend=True
                )
                
                st.plotly_chart(fig, use_container_width=True)

def show_model_comparison():
    st.header("ğŸ“Š Comparaison des ModÃ¨les")
    
    if not st.session_state.get('models_trained', False):
        st.warning("âš ï¸ Veuillez d'abord entraÃ®ner les modÃ¨les dans l'onglet 'EntraÃ®nement'")
        return
    
    results = st.session_state.models_results
    
    # Tableau de comparaison
    st.subheader("ğŸ“‹ Tableau Comparatif")
    
    comparison_data = []
    for name, result in results.items():
        comparison_data.append({
            'ModÃ¨le': name,
            'RÂ² Train': result['train_metrics']['R2'],
            'RÂ² Test': result['test_metrics']['R2'],
            'RMSE Train': result['train_metrics']['RMSE'],
            'RMSE Test': result['test_metrics']['RMSE'],
            'MAE Train': result['train_metrics']['MAE'],
            'MAE Test': result['test_metrics']['MAE'],
            'Temps (s)': result['training_time']
        })
    
    comparison_df = pd.DataFrame(comparison_data)
    st.dataframe(comparison_df.round(4), use_container_width=True)
    
    # Graphiques de comparaison
    st.subheader("ğŸ“ˆ Visualisation des Performances")
    
    metric = st.selectbox("MÃ©trique Ã  comparer:", ['RÂ² Test', 'RMSE Test', 'MAE Test', 'Temps (s)'])
    
    fig = px.bar(
        comparison_df,
        x='ModÃ¨le',
        y=metric,
        title=f"Comparaison des ModÃ¨les - {metric}",
        color=metric,
        color_continuous_scale='Viridis'
    )
    
    fig.update_layout(xaxis_tickangle=-45)
    st.plotly_chart(fig, use_container_width=True)
    
    # Matrice de corrÃ©lation des prÃ©dictions
    st.subheader("ğŸ”— CorrÃ©lation des PrÃ©dictions")
    
    # Collecte des prÃ©dictions de test
    predictions_df = pd.DataFrame()
    for name, result in results.items():
        predictions_df[name] = result['predictions']['test']
    
    corr_matrix = predictions_df.corr()
    
    fig_heatmap = go.Figure(data=go.Heatmap(
        z=corr_matrix.values,
        x=corr_matrix.columns,
        y=corr_matrix.columns,
        colorscale='RdBu_r',
        zmin=-1,
        zmax=1,
        text=corr_matrix.round(3).values,
        texttemplate="%{text}",
        textfont={"size": 12}
    ))
    
    fig_heatmap.update_layout(
        title="CorrÃ©lation entre les PrÃ©dictions des ModÃ¨les",
        width=600,
        height=600
    )
    
    st.plotly_chart(fig_heatmap, use_container_width=True)

def show_recommendations():
    st.header("ğŸ† Recommandations de ModÃ¨le")
    
    if not st.session_state.get('models_trained', False):
        st.warning("âš ï¸ Veuillez d'abord entraÃ®ner les modÃ¨les dans l'onglet 'EntraÃ®nement'")
        return
    
    results = st.session_state.models_results
    
    # Trouver le meilleur modÃ¨le basÃ© sur RÂ²
    best_model_name = max(results.items(), key=lambda x: x[1]['test_metrics']['R2'])[0]
    best_model_r2 = results[best_model_name]['test_metrics']['R2']
    
    # Trouver le modÃ¨le le plus rapide
    fastest_model_name = min(results.items(), key=lambda x: x[1]['training_time'])[0]
    fastest_model_time = results[fastest_model_name]['training_time']
    
    # Affichage des recommandations
    col1, col2 = st.columns(2)
    
    with col1:
        st.success(f"""
        **ğŸ¯ Meilleur ModÃ¨le (PrÃ©cision)**
        
        **{best_model_name}**
        
        ğŸ“Š RÂ² Score: **{best_model_r2:.4f}**
        ğŸ“ˆ RMSE: **{results[best_model_name]['test_metrics']['RMSE']:.2f}**
        â±ï¸ Temps: **{results[best_model_name]['training_time']:.2f}s**
        
        *RecommandÃ© pour la prÃ©cision maximale*
        """)
    
    with col2:
        st.info(f"""
        **âš¡ ModÃ¨le le Plus Rapide**
        
        **{fastest_model_name}**
        
        ğŸ“Š RÂ² Score: **{results[fastest_model_name]['test_metrics']['R2']:.4f}**
        ğŸ“ˆ RMSE: **{results[fastest_model_name]['test_metrics']['RMSE']:.2f}**
        â±ï¸ Temps: **{fastest_model_time:.2f}s**
        
        *RecommandÃ© pour les applications temps rÃ©el*
        """)
    
    # Analyse de trade-off
    st.subheader("ğŸ“Š Analyse de Trade-off")
    
    tradeoff_data = []
    for name, result in results.items():
        tradeoff_data.append({
            'ModÃ¨le': name,
            'RÂ² Test': result['test_metrics']['R2'],
            'Temps (s)': result['training_time']
        })
    
    tradeoff_df = pd.DataFrame(tradeoff_data)
    
    fig_tradeoff = px.scatter(
        tradeoff_df,
        x='Temps (s)',
        y='RÂ² Test',
        text='ModÃ¨le',
        title="Trade-off: PrÃ©cision vs Temps d'EntraÃ®nement",
        size_max=60
    )
    
    fig_tradeoff.update_traces(textposition='top center')
    fig_tradeoff.update_layout(
        xaxis_title="Temps d'EntraÃ®nement (secondes)",
        yaxis_title="RÂ² Score (Test)"
    )
    
    st.plotly_chart(fig_tradeoff, use_container_width=True)
    
    # SÃ©lection finale du modÃ¨le
    st.subheader("ğŸ¤” Quel ModÃ¨le Choisir?")
    
    selected_model = st.selectbox(
        "SÃ©lectionnez le modÃ¨le Ã  utiliser pour les prÃ©dictions:",
        list(results.keys()),
        index=list(results.keys()).index(best_model_name)
    )
    
    if st.button("âœ… Confirmer la SÃ©lection"):
        st.session_state.selected_model = selected_model
        st.session_state.best_model = results[selected_model]['model']
        st.success(f"âœ… ModÃ¨le **{selected_model}** sÃ©lectionnÃ© pour les prÃ©dictions!")
        
        # Sauvegarde du modÃ¨le
        try:
            joblib.dump(st.session_state.best_model, 'best_model.pkl')
            joblib.dump(st.session_state.scaler, 'scaler.pkl')
            st.info("ğŸ’¾ ModÃ¨le et scaler sauvegardÃ©s pour les prÃ©dictions")
        except Exception as e:
            st.error(f"âŒ Erreur lors de la sauvegarde: {str(e)}")

def show_model_management():
    st.header("ğŸ’¾ Gestion des ModÃ¨les")
    
    if not st.session_state.get('models_trained', False):
        st.warning("âš ï¸ Veuillez d'abord entraÃ®ner les modÃ¨les")
        return
    
    # Information sur les modÃ¨les entraÃ®nÃ©s
    st.subheader("ğŸ“‹ ModÃ¨les EntraÃ®nÃ©s")
    
    for name, result in st.session_state.models_results.items():
        with st.expander(f"ğŸ”§ {name}"):
            st.write(f"**Type:** {type(result['model']).__name__}")
            st.write(f"**ParamÃ¨tres:** {result['model'].get_params()}")
            
            # Bouton de tÃ©lÃ©chargement
            try:
                model_bytes = joblib.dumps(result['model'])
                st.download_button(
                    label=f"ğŸ“¥ TÃ©lÃ©charger {name}",
                    data=model_bytes,
                    file_name=f"{name.replace(' ', '_').lower()}.pkl",
                    mime="application/octet-stream"
                )
            except Exception as e:
                st.error(f"Erreur lors de la sÃ©rialisation: {str(e)}")
    
    # ModÃ¨le sÃ©lectionnÃ©
    if st.session_state.get('selected_model'):
        st.subheader("ğŸ¯ ModÃ¨le SÃ©lectionnÃ©")
        
        selected = st.session_state.selected_model
        st.success(f"**ModÃ¨le actuellement sÃ©lectionnÃ©:** {selected}")
        
        # MÃ©triques dÃ©taillÃ©es du modÃ¨le sÃ©lectionnÃ©
        results = st.session_state.models_results[selected]
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.metric("RÂ² Score (Test)", f"{results['test_metrics']['R2']:.4f}")
            st.metric("RMSE (Test)", f"{results['test_metrics']['RMSE']:.2f}")
        
        with col2:
            st.metric("MAE (Test)", f"{results['test_metrics']['MAE']:.2f}")
            st.metric("Temps d'entraÃ®nement", f"{results['training_time']:.2f}s")

if __name__ == "__main__":
    main()